Machine learning shares [[Triennale/Terzo anno/Reti di calcolatori/Sicurezza/Introduzione|vulnerabilities]] with traditional software.

| Attacker's capability \ Attacker's goal |                          Integrity                          |                                 Availability                                  |                     Privacy / Confidentiality                     |
| :-------------------------------------: | :---------------------------------------------------------: | :---------------------------------------------------------------------------: | :---------------------------------------------------------------: |
|              **Test data**              | - Evasion (i.e. adversarial examples)<br>- Prompt injection |                               - Sponge attacks                                | - Model extraction<br>- Model inversion<br>- Membership inference |
|            **Training data**            |                - Backdoor/targeted poisoning                | - Indiscriminate (DoS) poisoning to maximize test error<br>- Sponge poisoning |       - Training attacks to make the model memorize better        |

### Adversarial examples
These type of attacks expose model **existing weaknesses**.
Adversarial examples are carefully crafted inputs designed to mislead model predictions.

They are generated by gradually adjusting the input to make the model more confident in the wrong class.

![[Adversarial example.png|500]]

In the case of computer vision, the adversarial examples can also be **indistinguishable to human eyes**.

To keep adversarial changes imperceptible, [lp norms](https://en.wikipedia.org/wiki/Lp_space) are used to quantify modifications.

In the case of natural language processing, replacing a character with an adversarial one, could lead the model to a wrong decision.

Another example is the [[Tecniche anti-spam|anti-spam filter]], here a model sums the weights of words inside of the email to establish if it is spam or not.
If the spammer adds legitimate words, it can weaken the effectiveness of the filter.

### Jailbreak attacks on LLMs
LLMs are typically aligned through reinforcement learning from human feedback.
Jailbreak attacks are used to circumvent LLM's safety mechanism to generate **harmful responses**.

### Poisoning attacks
Data poisoning attacks assume the capacity of the attacker to **influence the training phase** to drive the model toward unexpected missclassification at test time.

The attacker creates **spurious correlations**, that is connection between variables that appear to be causal, but it is not.

This technique can be used against anti-spam filters, by flooding them with emails that appear legitimate, over time this **degrades the filter's accuracy** allowing more spam to bypass detection and overwhelm users.

It is also important to notice that if a dataset is poisoned, every model trained on it will be compromised.

Another poisoning attack is called **sponge poisoning**, it increases energy consumption and latency in machine learning models by subtly modifying the training process, all while maintaining high accuracy.
This is done (for example) by _modifying the weight update formula_, in order to calculate irrelevant gradients and rendering hardware accelerators useless if done in a certain way.

### Proactive defenses
**Training data sanitization** aims to remove potentially-harmful training points before the training phase.
>Poisoning samples typically exhibit outlying behavior.

**Data augmentation** can reduce the percentage of poisoning samples, and finally **pruning** and can be used to sanitize the model.
