The term **artificial intelligence** is applied when a machine _mimics_ cognitive functions that humans associate with human minds such as **learning** and **problem solving**.

### The story so far
- 1943: McCulloch and Pitts propose a model for an artificial neuron
- 1950: Alan Turing developed the [Turing test](https://en.wikipedia.org/wiki/Turing_test)
- 1956: Artificial intelligence was accepted as a field at the Dartmouth conference
- 1957: Frank Rosenblat invented the **perceptron**
- 1986: Rumelhart, Hinton, and WIlliams introduce the **backpropagation** algorith with which multi-layer perceptron can be trained

In the early 90' the [[Regressione lineare#Support Vector Machine (SVM)|SVM]] was developed and thanks to the _kernel trick_ it was possible to classify non linear problems, then machine learning had successes in many areas such as speach recognition and robotics.

The use of deeper [[Triennale/Terzo anno/Data and web mining/Reti neurali/Introduzione|neural networks]] caused a "rebranding" of the term, so we began talking about **deep learning**.

It was in 2012 that **AlexNet** first appeared by achieving a top-5 error of 15.3% in the **ImageNet challenge**, more than $10.8$ percentage points lower than that of the runner up.

This was made feasible due to the use of **GPUs** during training, thus becoming an essential ingredient of the deep learning revolution.
